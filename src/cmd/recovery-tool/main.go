package main

import (
	"compress/gzip"
	"encoding/binary"
	"flag"
	"fmt"
	"io"
	"log"
	"os"
	"path/filepath"

	"github.com/klauspost/compress/zstd"
	"github.com/pmalasek/cumulus3/src/internal/storage"
)

// BlobLocation dr≈æ√≠ informaci, kde naj√≠t data pro dan√© BlobID
type BlobLocation struct {
	VolumePath     string
	Offset         int64
	SizeCompressed int64
	CompAlg        uint8
}

func main() {
	dataPath := flag.String("src", "./data", "Cesta ke zdrojov√Ωm dat≈Øm (kde jsou volume_*.dat a files.bin)")
	restorePath := flag.String("dst", "./restored", "Cesta, kam se maj√≠ obnovit soubory")
	flag.Parse()

	if *dataPath == "" || *restorePath == "" {
		flag.Usage()
		os.Exit(1)
	}

	fmt.Println("üîç Zaƒç√≠n√°m anal√Ωzu volume soubor≈Ø...")
	blobMap, err := scanVolumes(*dataPath)
	if err != nil {
		log.Fatalf("Chyba p≈ôi skenov√°n√≠ volumes: %v", err)
	}
	fmt.Printf("‚úÖ Nalezeno %d unik√°tn√≠ch blob≈Ø.\n", len(blobMap))

	fmt.Println("üìÇ Zaƒç√≠n√°m obnovu soubor≈Ø z files.bin...")
	count, err := restoreFiles(*dataPath, *restorePath, blobMap)
	if err != nil {
		log.Fatalf("Chyba p≈ôi obnovƒõ: %v", err)
	}

	fmt.Printf("üéâ Hotovo! Obnoveno %d soubor≈Ø do '%s'.\n", count, *restorePath)
}

// scanVolumes projde v≈°echny .dat soubory a zaindexuje bloby
func scanVolumes(dir string) (map[int64]BlobLocation, error) {
	index := make(map[int64]BlobLocation)

	files, err := filepath.Glob(filepath.Join(dir, "volume_*.dat"))
	if err != nil {
		return nil, err
	}

	for _, file := range files {
		baseName := filepath.Base(file)
		metaName := baseName[:len(baseName)-4] + ".meta" // volume_1.dat -> volume_1.meta
		metaPath := filepath.Join(dir, metaName)

		// Zkus√≠me pou≈æ√≠t META soubor pro rychl√© skenov√°n√≠
		if _, err := os.Stat(metaPath); err == nil {
			fmt.Printf("  -> Rychl√© skenov√°n√≠ pomoc√≠ %s\n", metaName)
			if err := scanMetaFile(file, metaPath, index); err == nil {
				continue // √öspƒõch, jdeme na dal≈°√≠ volume
			}
			log.Printf("Varov√°n√≠: Chyba p≈ôi ƒçten√≠ %s, p≈ôech√°z√≠m na pomal√© skenov√°n√≠ .dat: %v", metaName, err)
		}

		fmt.Printf("  -> Pomal√© skenov√°n√≠ %s (chyb√≠ nebo vadn√Ω .meta)\n", baseName)
		scanDatFile(file, index)
	}

	return index, nil
}

func scanMetaFile(volPath, metaPath string, index map[int64]BlobLocation) error {
	f, err := os.Open(metaPath)
	if err != nil {
		return err
	}
	defer f.Close()

	recordSize := 29 // BlobID(8) + Offset(8) + Size(8) + Comp(1) + CRC(4)
	buf := make([]byte, recordSize)

	for {
		if _, err := io.ReadFull(f, buf); err != nil {
			if err == io.EOF {
				break
			}
			return err
		}

		blobID := int64(binary.BigEndian.Uint64(buf[0:8]))
		offset := int64(binary.BigEndian.Uint64(buf[8:16]))
		size := int64(binary.BigEndian.Uint64(buf[16:24]))
		compAlg := buf[24]
		// crc := binary.BigEndian.Uint32(buf[25:29])

		// Offset v meta souboru ukazuje na zaƒç√°tek hlaviƒçky v .dat souboru.
		// Pro ƒçten√≠ dat pot≈ôebujeme p≈ôeskoƒçit hlaviƒçku (HeaderSize).
		// Ale pozor: Store.WriteBlob vrac√≠ offset zaƒç√°tku hlaviƒçky.
		// A na≈°e struktura BlobLocation oƒçek√°v√° offset zaƒç√°tku DAT.
		// Tak≈æe mus√≠me p≈ôiƒç√≠st HeaderSize.

		// HeaderSize mus√≠me importovat nebo definovat. Zde natvrdo 22 (4+1+1+8+8).
		const HeaderSize = 22

		index[blobID] = BlobLocation{
			VolumePath:     volPath,
			Offset:         offset + int64(HeaderSize),
			SizeCompressed: size,
			CompAlg:        compAlg,
		}
	}
	return nil
}

func scanDatFile(file string, index map[int64]BlobLocation) {
	f, err := os.Open(file)
	if err != nil {
		log.Printf("Varov√°n√≠: Nelze otev≈ô√≠t %s: %v", file, err)
		return
	}
	defer f.Close()

	// Proch√°z√≠me soubor blok po bloku
	for {
		// Z√≠sk√°me aktu√°ln√≠ offset (zaƒç√°tek hlaviƒçky)
		offset, _ := f.Seek(0, io.SeekCurrent)

		// ƒåteme hlaviƒçku
		header := make([]byte, storage.HeaderSize)
		if _, err := io.ReadFull(f, header); err != nil {
			if err == io.EOF {
				break // Konec souboru
			}
			log.Printf("Chyba ƒçten√≠ hlaviƒçky v %s: %v", file, err)
			break
		}

		magic := binary.BigEndian.Uint32(header[0:4])
		if magic != uint32(storage.MagicBytes) {
			log.Printf("Chyba: Neplatn√Ω magic number na offsetu %d v %s. P≈ôeskakuji zbytek souboru.", offset, file)
			break
		}

		// ver := header[4]
		compAlg := header[5]
		size := int64(binary.BigEndian.Uint64(header[6:14]))
		blobID := int64(binary.BigEndian.Uint64(header[14:22]))

		// Ulo≈æ√≠me do indexu (offset ukazuje na zaƒç√°tek dat, tj. za hlaviƒçkou)
		index[blobID] = BlobLocation{
			VolumePath:     file,
			Offset:         offset + int64(storage.HeaderSize),
			SizeCompressed: size,
			CompAlg:        compAlg,
		}

		// P≈ôeskoƒç√≠me data a patiƒçku
		if _, err := f.Seek(size+int64(storage.FooterSize), io.SeekCurrent); err != nil {
			break
		}
	}
}

// restoreFiles ƒçte files.bin a obnovuje soubory
func restoreFiles(srcDir, dstDir string, blobIndex map[int64]BlobLocation) (int, error) {
	logPath := filepath.Join(srcDir, "files.bin")
	f, err := os.Open(logPath)
	if err != nil {
		return 0, fmt.Errorf("nelze otev≈ô√≠t files.bin: %w", err)
	}
	defer f.Close()

	if err := os.MkdirAll(dstDir, 0755); err != nil {
		return 0, err
	}

	restoredCount := 0
	decoder, _ := zstd.NewReader(nil)
	defer decoder.Close()

	for {
		// 1. P≈ôeƒç√≠st d√©lku z√°znamu
		lenBuf := make([]byte, 4)
		if _, err := io.ReadFull(f, lenBuf); err != nil {
			if err == io.EOF {
				break
			}
			return restoredCount, err
		}
		recordLen := binary.BigEndian.Uint32(lenBuf)

		// 2. P≈ôeƒç√≠st cel√Ω z√°znam
		record := make([]byte, recordLen)
		if _, err := io.ReadFull(f, record); err != nil {
			return restoredCount, err
		}

		// 3. Parsovat z√°znam (reverzn√≠ in≈æen√Ωrstv√≠ logger.go)
		// ID Len (2)
		idLen := binary.BigEndian.Uint16(record[0:2])
		// ID (idLen)
		// id := string(record[2 : 2+idLen])
		cursor := 2 + int(idLen)

		// BlobID (8)
		blobID := int64(binary.BigEndian.Uint64(record[cursor : cursor+8]))
		cursor += 8

		// CreatedAt (8)
		cursor += 8

		// Flags (1)
		flags := record[cursor]
		cursor += 1

		// Optional fields based on flags
		if flags&(1<<0) != 0 { // OldCumulusID
			cursor += 8
		}
		if flags&(1<<1) != 0 { // ExpiresAt
			cursor += 8
		}

		// Name Len (2)
		nameLen := binary.BigEndian.Uint16(record[cursor : cursor+2])
		cursor += 2

		// Name
		filename := string(record[cursor : cursor+int(nameLen)])

		// 4. Obnovit soubor
		loc, exists := blobIndex[blobID]
		if !exists {
			log.Printf("‚ùå Chyba: BlobID %d pro soubor '%s' nebyl nalezen ve volumech.", blobID, filename)
			continue
		}

		if err := extractFile(dstDir, filename, loc, decoder); err != nil {
			log.Printf("‚ùå Chyba p≈ôi extrakci '%s': %v", filename, err)
		} else {
			// fmt.Printf("Obnoven: %s\n", filename)
			restoredCount++
		}
	}

	return restoredCount, nil
}

func extractFile(dstDir, filename string, loc BlobLocation, zstdDecoder *zstd.Decoder) error {
	// Otev≈ô√≠t volume
	vol, err := os.Open(loc.VolumePath)
	if err != nil {
		return err
	}
	defer vol.Close()

	// Skoƒçit na data
	if _, err := vol.Seek(loc.Offset, 0); err != nil {
		return err
	}

	// Omezit ƒçten√≠ jen na velikost blobu
	limitReader := io.LimitReader(vol, loc.SizeCompressed)

	// P≈ôipravit v√Ωstupn√≠ soubor
	outPath := filepath.Join(dstDir, filename)
	outFile, err := os.Create(outPath)
	if err != nil {
		return err
	}
	defer outFile.Close()

	// Dekomprese
	switch loc.CompAlg {
	case 0: // None
		_, err = io.Copy(outFile, limitReader)
	case 1: // Gzip
		gz, err := gzip.NewReader(limitReader)
		if err != nil {
			return err
		}
		defer gz.Close()
		_, err = io.Copy(outFile, gz)
	case 2: // Zstd
		if err := zstdDecoder.Reset(limitReader); err != nil {
			return err
		}
		_, err = io.Copy(outFile, zstdDecoder)
	default:
		return fmt.Errorf("nezn√°m√° komprese: %d", loc.CompAlg)
	}

	return err
}
